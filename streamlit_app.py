import streamlit as st
import google.generativeai as genai
import chromadb
import os
import shutil
import config
from utils import GeminiEmbeddingFunction
# ingest.py ã‹ã‚‰PDFå‡¦ç†é–¢æ•°ã‚’èª­ã¿è¾¼ã‚€
from ingest import load_and_chunk_pdf

# --- è¨­å®šã¨åˆæœŸåŒ– ---
st.set_page_config(page_title="Research AI Assistant", page_icon="", layout="wide")

# APIè¨­å®š
genai.configure(api_key=config.GOOGLE_API_KEY)

# --- ãƒªã‚½ãƒ¼ã‚¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŒ– ---
@st.cache_resource
def load_db_and_model():
    try:
        gemini_ef = GeminiEmbeddingFunction()
        client = chromadb.PersistentClient(path=config.DB_DIR)
        collection = client.get_or_create_collection(
            name=config.COLLECTION_NAME, 
            embedding_function=gemini_ef
        )
        model = genai.GenerativeModel(config.GENERATION_MODEL)
        return collection, model, gemini_ef
    except Exception as e:
        st.error(f"DBã‚¨ãƒ©ãƒ¼: {e}")
        return None, None, None

collection, model, gemini_ef = load_db_and_model()

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šæ–‡çŒ®ç®¡ç†æ©Ÿèƒ½ ---
with st.sidebar:
    st.markdown("---")
    show_debug = st.checkbox("ãƒ‡ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã‚’è¡¨ç¤º", value=False)

    st.header("æ–‡çŒ®ç®¡ç†")
    
    # 1. æ–°è¦PDFè¿½åŠ 
    st.subheader("æ–°è¦PDFã®è¿½åŠ ")
    uploaded_files = st.file_uploader("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"], accept_multiple_files=True)
    
    if uploaded_files and st.button("è¿½åŠ ã—ã¦å­¦ç¿’é–‹å§‹"):
        if collection:
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, uploaded_file in enumerate(uploaded_files):
                status_text.text(f"å‡¦ç†ä¸­: {uploaded_file.name}...")
                
                # A. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                save_path = os.path.join(config.PDF_SOURCE_DIR, uploaded_file.name)
                with open(save_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                # B. ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã¨ãƒãƒ£ãƒ³ã‚¯åŒ– (ingest.pyã®é–¢æ•°ã‚’åˆ©ç”¨)
                try:
                    chunks = load_and_chunk_pdf(save_path)
                    
                    # C. DBã¸ã®ç™»éŒ²
                    documents = [chunk.page_content for chunk in chunks]
                    metadatas = [{"source": uploaded_file.name, "page_chunk": idx} for idx, chunk in enumerate(chunks)]
                    ids = [f"{uploaded_file.name}_{idx}" for idx in range(len(chunks))]
                    
                    if documents:
                        collection.add(documents=documents, metadatas=metadatas, ids=ids)
                        st.success(f"âœ… {uploaded_file.name} ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ã—ã¾ã—ãŸ")
                    
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ ({uploaded_file.name}): {e}")
                
                progress_bar.progress((i + 1) / len(uploaded_files))
            
            status_text.text("ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            st.rerun() # ç”»é¢ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒªã‚¹ãƒˆã‚’æ›´æ–°

    st.markdown("---")

    # 2. ç™»éŒ²æ¸ˆã¿æ–‡çŒ®ãƒªã‚¹ãƒˆè¡¨ç¤º
    st.subheader("ç™»éŒ²æ¸ˆã¿æ–‡çŒ®ä¸€è¦§")
    if collection:
        # DBã‹ã‚‰å…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿æŠ½å‡º
        # (ä»¶æ•°ãŒå¤šã„å ´åˆã¯é‡ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ä¸Šé™ã‚’è¨­ã‘ã‚‹ã‹ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã™ãŒã€å€‹äººåˆ©ç”¨ãªã‚‰ã“ã‚Œã§OK)
        try:
            all_data = collection.get(include=["metadatas"])
            unique_sources = set()
            for meta in all_data["metadatas"]:
                if meta and "source" in meta:
                    unique_sources.add(meta["source"])
            
            if unique_sources:
                for source in sorted(list(unique_sources)):
                    st.markdown(f"-  {source}")
                st.caption(f"åˆè¨ˆ: {len(unique_sources)} ãƒ•ã‚¡ã‚¤ãƒ«")
            else:
                st.info("ã¾ã æ–‡çŒ®ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        except Exception as e:
            st.error("ãƒªã‚¹ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ï¼šãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ ---
st.title("ğŸ¤– Research AI Assistant")
st.caption(f"Powered by {config.GENERATION_MODEL}")

# å±¥æ­´ç®¡ç†
if "messages" not in st.session_state:
    st.session_state.messages = []

# å±¥æ­´è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "sources" in message and message["sources"]:
            with st.expander("å‚ç…§æ–‡çŒ®"):
                st.write(message["sources"])

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
st.markdown("---")
with st.form(key="query_form", clear_on_submit=True):
    user_input = st.text_area("è³ªå•ã‚’å…¥åŠ›:", height=100, placeholder="è³ªå•ã‚’å…¥åŠ›...\n(Ctrl+Enter ã§é€ä¿¡)")
    col1, col2 = st.columns([1, 6])
    with col1:
        submit_button = st.form_submit_button("é€ä¿¡", type="primary")

# å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯
if submit_button and user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.rerun()

if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
    prompt = st.session_state.messages[-1]["content"]
    
    if collection and model:
        with st.chat_message("assistant"):
            msg_placeholder = st.empty()
            
            with st.spinner("æ–‡çŒ®ã‚’æ¤œç´¢ä¸­..."):
                query_vector = gemini_ef.embed_query(prompt)
                results = collection.query(
                    query_embeddings=[query_vector],
                    n_results=20, # æ–‡çŒ®ã‚’å¤šã‚ã«å–å¾—
                    include=["documents", "metadatas", "distances"]
                )
            
            if show_debug:
                with st.expander(" ãƒ‡ãƒãƒƒã‚°: æ¤œç´¢ã•ã‚ŒãŸç”Ÿãƒ‡ãƒ¼ã‚¿ (Embeddingç¢ºèª)", expanded=True):
                    st.write(f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {prompt}")
                    
                    if results['documents'] and results['documents'][0]:
                        for i, doc in enumerate(results['documents'][0]):
                            score = results['distances'][0][i]
                            meta = results['metadatas'][0][i]
                            src = meta.get('source', 'Unknown')
                            
                            # ã‚¹ã‚³ã‚¢ã¨å†…å®¹ã‚’è¡¨ç¤º
                            st.markdown(f"**Rank {i+1}** |  `{src}` |  Distance: `{score:.4f}`")
                            st.text(doc[:150] + "...") # é•·ã„ã®ã§å…ˆé ­ã ã‘è¡¨ç¤º
                            st.divider()
                    else:
                        st.warning("æ¤œç´¢çµæœãŒ0ä»¶ã§ã™ã€‚ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãŒã†ã¾ãã„ã£ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

            context_text = ""
            sources = set()
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    meta = results['metadatas'][0][i]
                    src = meta.get('source', 'Unknown')
                    sources.add(src)
                    context_text += f"<doc source='{src}'>\n{doc}\n</doc>\n\n"
            
            system_prompt = f"""
            ã‚ãªãŸã¯å„ªç§€ãªç ”ç©¶ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
            ä»¥ä¸‹ã®ã€Œæ¤œç´¢ã•ã‚ŒãŸæ–‡çŒ®ãƒ‡ãƒ¼ã‚¿ã€ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
            
            ã€é‡è¦äº‹é …ã€‘
            - ãƒ‡ãƒ¼ã‚¿ã¯PDFã‹ã‚‰è‡ªå‹•æŠ½å‡ºã•ã‚ŒãŸã‚‚ã®ã§ã€èª¤å­—ã‚„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå´©ã‚ŒãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
            - è¤‡æ•°ã®æ–‡çŒ®ã«æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯çµ±åˆã—ã¦ç­”ãˆã¦ãã ã•ã„ã€‚
            - æ–‡çŒ®ã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€æ­£ç›´ã«ã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

            ã€æ–‡çŒ®ãƒ‡ãƒ¼ã‚¿ã€‘
            {context_text}

            ã€è³ªå•ã€‘
            {prompt}
            """

            try:
                full_response = ""
                response = model.generate_content(system_prompt, stream=True)
                
                for chunk in response:
                    full_response += chunk.text
                    msg_placeholder.markdown(full_response + "â–Œ")
                
                msg_placeholder.markdown(full_response)
                
                if sources:
                    with st.expander("ä»Šå›ã®å›ç­”ã«ä½¿ç”¨ã—ãŸæ–‡çŒ®"):
                        st.write(list(sources))

                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": full_response,
                    "sources": list(sources)
                })

            except Exception as e:
                st.error(f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")